{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y2/lkm0p1s979jf_rsdx_9cgc0w0000gn/T/ipykernel_39702/741520124.py:76: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1aidzjezue/croot/pytorch_1687856425340/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lstm (114897, 100, 54)\n",
      "model_name :  LSTM input_size :  54 hidden_size :  30 num_classes :  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.utils import to_categorical\n",
    "import models\n",
    "import random\n",
    "import os\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(42)  # Seed 고정\n",
    "\n",
    "data=pd.read_csv('/Users/haseung-won/Desktop/학교/연구실/time_series_data/PAMAP2_Dataset/train.csv')\n",
    "test_data=pd.read_csv('/Users/haseung-won/Desktop/학교/연구실/time_series_data/PAMAP2_Dataset/test.csv')\n",
    "\n",
    "data = data.fillna(0)\n",
    "test_data = test_data.fillna(0)\n",
    "X = data.drop(columns=['activityID'])\n",
    "y = data['activityID']\n",
    "\n",
    "test_X= test_data.drop(columns=['activityID'])\n",
    "test_y= test_data['activityID']\n",
    "\n",
    "num_classes =25\n",
    "\n",
    "timesteps = 100\n",
    "step_size = 20\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# 좌우로 10개의 타임스텝을 겹치게 하려면 step_size를 10으로 설정\n",
    "\n",
    "for i in range(0, len(X) - timesteps + 1, step_size):\n",
    "    X_sequence = X.iloc[i:i + timesteps, :].values\n",
    "    y_label = y.iloc[i + timesteps - 1]  # 시퀀스의 마지막 레이블 가져오기\n",
    "    X_train.append(X_sequence)\n",
    "    y_train.append(y_label)\n",
    "\n",
    "valid_index_range = len(test_X) - timesteps + 1\n",
    "\n",
    "for i in range(0, valid_index_range, step_size):\n",
    "    X_sequence = test_X.iloc[i:i + timesteps, :].values\n",
    "    y_label = test_y.iloc[i + timesteps - 1]  # 시퀀스의 마지막 레이블 가져오기\n",
    "    X_test.append(X_sequence)\n",
    "    y_test.append(y_label)\n",
    "\n",
    "\n",
    "X_train_lstm = np.array(X_train)\n",
    "y_train_lstm = np.array(y_train)\n",
    "\n",
    "X_test_lstm = np.array(X_test)\n",
    "y_test_lstm = np.array(y_test)\n",
    "\n",
    "\n",
    "# Now you can proceed with multi-label encoding\n",
    "y_train_encoded = np.array([to_categorical(labels, num_classes=num_classes) for labels in y_train])\n",
    "y_train_encoded_flattened = y_train_encoded.reshape(-1, num_classes)\n",
    "\n",
    "y_test_encoded = np.array([to_categorical(labels, num_classes=num_classes) for labels in y_test])\n",
    "y_test_encoded_flattened = y_test_encoded.reshape(-1, num_classes)\n",
    "\n",
    "# Assuming X_train_lstm, y_train_encoded_flattened are already prepared\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_encoded_flattened_tensor = torch.tensor(y_train_encoded_flattened, dtype=torch.float32)\n",
    "\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_encoded_flattened_tensor = torch.tensor(y_test_encoded_flattened, dtype=torch.float32)\n",
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, edge_index):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "print(\"X_train_lstm\",X_train_lstm.shape)\n",
    "num_nodes = X_train_lstm.shape[1]\n",
    "\n",
    "# Create an adjacency matrix with edges connecting each node to its neighbors\n",
    "adjacency_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "\n",
    "# Connect each node to its immediate neighbors (adjust as needed)\n",
    "for i in range(num_nodes):\n",
    "    if i > 0:\n",
    "        adjacency_matrix[i, i - 1] = 1.0\n",
    "    if i < num_nodes - 1:\n",
    "        adjacency_matrix[i, i + 1] = 1.0\n",
    "\n",
    "# Convert the adjacency matrix to a sparse tensor\n",
    "edge_index = torch.tensor(np.array(np.where(adjacency_matrix == 1)), dtype=torch.long).to(device)\n",
    "\n",
    "# Create instances of custom dataset\n",
    "custom_dataset = CustomDataset(X_train_tensor, y_train_encoded_flattened_tensor,edge_index)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Assuming you have a validation dataset named 'val_dataset'\n",
    "val_dataset = CustomDataset(X_test_tensor, y_test_encoded_flattened_tensor, edge_index)  # X_val_tensor, y_val_encoded_flattened_tensor, edge_index_val은 검증 데이터에 대한 텐서와 엣지 인덱스입니다.\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize model and hyperparameters\n",
    "input_size = X_train_tensor.shape[2]\n",
    "hidden_size = 30\n",
    "num_classes = 25\n",
    "\n",
    "model_name='LSTM'\n",
    "\n",
    "if model_name=='LSTM':\n",
    "    model=models.LSTMModel(input_size, hidden_size, num_classes).to(device)\n",
    "    lr = 0.00001\n",
    "elif model_name=='Conv1D':\n",
    "    model=models.Conv1DModel(input_size, num_classes).to(device)\n",
    "    lr = 0.0001\n",
    "elif model_name=='GCN':\n",
    "    model=models.GCNModel(input_size,hidden_size, num_classes).to(device)\n",
    "    edge_index=custom_dataset.edge_index\n",
    "    edge_index=edge_index.to(device)\n",
    "    lr = 0.0001\n",
    "elif model_name=='GCN2':\n",
    "    model=models.GCNModel2(input_size,hidden_size, num_classes).to(device)\n",
    "    edge_index=custom_dataset.edge_index\n",
    "    edge_index=edge_index.to(device)\n",
    "    lr = 0.0001\n",
    "elif model_name==\"AutoConv\":\n",
    "    lr = 0.0001\n",
    "    latent_size = 16  # Adjust as needed\n",
    "    model = models.ConvAutoencoder(input_size, latent_size).to(device)\n",
    "elif model_name=='Generate':\n",
    "    lr = 0.001\n",
    "    latent_size = 16  # Adjust as needed\n",
    "    model = models.AutoConvWithGenerator(input_size, latent_size,num_classes).to(device)\n",
    "\n",
    "print(\"model_name : \",model_name,\"input_size : \",input_size,\"hidden_size : \",hidden_size,\"num_classes : \",num_classes)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "from tqdm import tqdm  # Import tqdm library\n",
    "\n",
    "\n",
    "def some_custom_loss(generator_label_output,target):\n",
    "    # Define your custom loss here\n",
    "    # For example, you can use Mean Squared Error (MSE) loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    loss = mse_loss(generator_label_output, target)  # Define 'target' according to your task\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}] Training\", leave=False)\n",
    "\n",
    "    for inputs, labels in train_loader_tqdm:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if 'GCN' in model_name:\n",
    "            outputs = model(inputs,edge_index)\n",
    "            loss = criterion(outputs, labels)\n",
    "        elif model_name==\"AutoConv\":\n",
    "            target = inputs[:, -1, :]  # Use the last time step as the target for autoencoder\n",
    "            reconstructed_outputs = model(inputs)\n",
    "            # Select the last time step from reconstructed_outputs\n",
    "            reconstructed_outputs = reconstructed_outputs[:, -1, :]\n",
    "            loss = criterion(reconstructed_outputs, target)\n",
    "        elif model_name == \"Generate\":\n",
    "            target = inputs[:, -1, :]  # Use the last time step as the target for autoencoder\n",
    "\n",
    "            # 모델의 출력 중에서 decoder_outputs와 generator_label_output을 얻습니다.\n",
    "            decoder_outputs, generator_label_output = model(inputs)\n",
    "\n",
    "            # Select the last time step from decoder_outputs\n",
    "            decoder_outputs = decoder_outputs[:, -1, :]\n",
    "\n",
    "            # Autoencoder 부분의 손실 계산 (예: 평균 제곱 오차)\n",
    "            label_loss = criterion(generator_label_output, labels)\n",
    "\n",
    "            # Custom loss 함수인 some_custom_loss를 이용해 추가적인 손실 계산\n",
    "            custom_loss = some_custom_loss(decoder_outputs, target)\n",
    "\n",
    "            # Autoencoder 손실과 추가적인 손실을 합하여 총 손실 계산\n",
    "            loss = label_loss + custom_loss\n",
    "\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            if 'GCN' in model_name:\n",
    "                outputs = model(inputs, edge_index)\n",
    "                loss = criterion(outputs, labels)\n",
    "            elif model_name == \"AutoConv\":\n",
    "                target = inputs[:, -1, :]  # Use the last time step as the target for autoencoder\n",
    "                reconstructed_outputs = model(inputs)\n",
    "                # Select the last time step from reconstructed_outputs\n",
    "                reconstructed_outputs = reconstructed_outputs[:, -1, :]\n",
    "                loss = criterion(reconstructed_outputs, target)\n",
    "            elif model_name == \"Generate\":\n",
    "                target = inputs[:, -1, :]  # Use the last time step as the target for autoencoder\n",
    "\n",
    "                # 모델의 출력 중에서 decoder_outputs와 generator_label_output을 얻습니다.\n",
    "                decoder_outputs, generator_label_output = model(inputs)\n",
    "\n",
    "                # Select the last time step from decoder_outputs\n",
    "                decoder_outputs = decoder_outputs[:, -1, :]\n",
    "\n",
    "                # Autoencoder 부분의 손실 계산 (예: 평균 제곱 오차)\n",
    "                label_loss = criterion(generator_label_output, labels)\n",
    "\n",
    "                # Custom loss 함수인 some_custom_loss를 이용해 추가적인 손실 계산\n",
    "                custom_loss = some_custom_loss(decoder_outputs, target)\n",
    "\n",
    "                # Autoencoder 손실과 추가적인 손실을 합하여 총 손실 계산\n",
    "                loss = label_loss + custom_loss\n",
    "                print(f\"Label Loss: {label_loss:.4f}, Custom Loss: {custom_loss:.4f}\")\n",
    "\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), f\"model/{model_name}_timeseries_model.pth\")\n",
    "print(\"Model saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
