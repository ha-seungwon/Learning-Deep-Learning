{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# ##.dat 파일이 있는 디렉토리 지정\n",
    "# dir_dat = '/Users/haseung-won/Desktop/학교/연구실/time_series_data/PAMAP2_Dataset/Optional/'\n",
    "# dir_csv = '/Users/haseung-won/Desktop/학교/연구실/time_series_data/PAMAP2_Dataset/Optional_csv/'\n",
    "# ##디렉토리 내의 모든 파일을 순회\n",
    "# for filename in os.listdir(dir_dat):\n",
    "#     ## .dat 파일인 경우만 처리\n",
    "#     if filename.endswith('.dat'):\n",
    "#         ## .dat 파일 읽기 (구분자가 공백인 경우)\n",
    "#         data = pd.read_csv(os.path.join(dir_dat, filename), sep=\"\\s+\", header=None)\n",
    "\n",
    "#         ## 새로운 .csv 파일 이름 생성 (.dat를 .csv로 변경)\n",
    "#         new_filename = os.path.splitext(filename)[0] + '.csv'\n",
    "\n",
    "#         ## .csv 파일로 저장\n",
    "#         data.to_csv(os.path.join(dir_csv, new_filename), index=False, header=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_list=[]\n",
    "for subject_id in range(1,10,1):\n",
    "\n",
    "    dir_csv = f'/Users/haseung-won/Desktop/학교/연구실/time_series_data/PAMAP2_Dataset/Protocol_csv/subject10{str(subject_id)}.csv'\n",
    "\n",
    "    # Define column names based on the provided structure\n",
    "    column_names = [\n",
    "        'timestamp',\n",
    "        'activityID',\n",
    "        'heart_rate',\n",
    "    ]\n",
    "\n",
    "    imu_sensors = [\n",
    "        'temperature',\n",
    "        'acceleration16g_x', 'acceleration16g_y', 'acceleration16g_z',\n",
    "        'acceleration6g_x', 'acceleration6g_y', 'acceleration6g_z',\n",
    "        'gyroscope_x', 'gyroscope_y', 'gyroscope_z',\n",
    "        'magnetometer_x', 'magnetometer_y', 'magnetometer_z',\n",
    "        'orientation_1', 'orientation_2', 'orientation_3','orientation_4'\n",
    "    ]\n",
    "\n",
    "    imu_parts = ['hand', 'chest', 'ankle']\n",
    "\n",
    "    # Add column names for IMU hand, chest, and ankle data\n",
    "    for part in imu_parts:\n",
    "        for sensor in imu_sensors:\n",
    "            column_names.append(f'IMU_{part}_{sensor}')\n",
    "    print(column_names)\n",
    "    print(len(column_names))\n",
    "    # Read the CSV file using pandas\n",
    "    df = pd.read_csv(dir_csv, names=column_names)\n",
    "    df_list.append(df)\n",
    "\n",
    "    # Now df contains the data from the CSV file with appropriate column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df_list=[]\n",
    "subject_id_list=[1,5,6,8,9]\n",
    "for subject_id in subject_id_list:\n",
    "\n",
    "    dir_csv = f'/Users/haseung-won/Desktop/학교/연구실/time_series_data/PAMAP2_Dataset/Optional_csv/subject10{str(subject_id)}.csv'\n",
    "\n",
    "    # Define column names based on the provided structure\n",
    "    column_names = [\n",
    "        'timestamp',\n",
    "        'activityID',\n",
    "        'heart_rate',\n",
    "    ]\n",
    "\n",
    "    imu_sensors = [\n",
    "        'temperature',\n",
    "        'acceleration16g_x', 'acceleration16g_y', 'acceleration16g_z',\n",
    "        'acceleration6g_x', 'acceleration6g_y', 'acceleration6g_z',\n",
    "        'gyroscope_x', 'gyroscope_y', 'gyroscope_z',\n",
    "        'magnetometer_x', 'magnetometer_y', 'magnetometer_z',\n",
    "        'orientation_1', 'orientation_2', 'orientation_3','orientation_4'\n",
    "    ]\n",
    "\n",
    "    imu_parts = ['hand', 'chest', 'ankle']\n",
    "\n",
    "    # Add column names for IMU hand, chest, and ankle data\n",
    "    for part in imu_parts:\n",
    "        for sensor in imu_sensors:\n",
    "            column_names.append(f'IMU_{part}_{sensor}')\n",
    "    print(column_names)\n",
    "    print(len(column_names))\n",
    "    # Read the CSV file using pandas\n",
    "    df = pd.read_csv(dir_csv, names=column_names)\n",
    "    test_df_list.append(df)\n",
    "\n",
    "    # Now df contains the data from the CSV file with appropriate column names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # 예시용 데이터 생성 (실제 데이터를 읽어오는 부분으로 대체해야 합니다)\n",
    "# data = df_list[0]\n",
    "\n",
    "# # 시간에 대한 열을 Datetime 형식으로 변환합니다.\n",
    "# data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n",
    "\n",
    "# # 색상 맵 설정 (activityID에 맞게 색상 지정)\n",
    "# num_activities = data['activityID'].nunique()\n",
    "# cmap = plt.get_cmap('viridis', num_activities)\n",
    "\n",
    "# # IMU 데이터 컬럼명 리스트\n",
    "# imu_columns = data.columns[3:]\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# for imu_column in imu_columns:\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "    \n",
    "#     for activity_id, group in data.groupby('activityID'):\n",
    "#         plt.plot(group['timestamp'], group[imu_column], label=f'Activity {activity_id}', color=cmap(activity_id - 1))\n",
    "\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.title(f'{imu_column} Time Series Data with Different Colors for ActivityID')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 데이터 병합\n",
    "data = df_list[0]\n",
    "\n",
    "\n",
    "data = data.fillna(0)\n",
    "X = data.drop(columns=['activityID'])\n",
    "y = data['activityID']\n",
    "num_classes =25\n",
    "n_features = X.shape[1]  # Number of columns in your feature matrix X\n",
    "print(num_classes)\n",
    "# 시퀀스 길이 설정\n",
    "timesteps = 10\n",
    "\n",
    "X_train_lstm = []\n",
    "y_train_lstm = []\n",
    "\n",
    "for i in range(len(X) - timesteps + 1):\n",
    "    X_sequence = X.iloc[i:i + timesteps, :].values\n",
    "    y_label = y.iloc[i + timesteps - 1]  # Get a single label for the sequence\n",
    "    X_train_lstm.append(X_sequence)\n",
    "    y_train_lstm.append(y_label)\n",
    "\n",
    "\n",
    "X_train_lstm = np.array(X_train_lstm)\n",
    "y_train_lstm = np.array(y_train_lstm)\n",
    "print(num_classes)\n",
    "# Now you can proceed with multi-label encoding\n",
    "y_train_encoded = np.array([to_categorical(labels, num_classes=num_classes) for labels in y_train_lstm])\n",
    "y_train_encoded_flattened = y_train_encoded.reshape(-1, num_classes)\n",
    "\n",
    "# LSTM 모델 구축\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(timesteps, n_features)))\n",
    "model.add(Dense(units=num_classes, activation='sigmoid'))  # Use 'sigmoid' for multi-label classification\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Use 'binary_crossentropy' for multi-label\n",
    "\n",
    "# 모델 학습\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "model.fit(X_train_lstm, y_train_encoded_flattened, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/Elstm_timeseries_model.h5\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_list[1] contains your evaluation data\n",
    "\n",
    "# Preprocessing on df_list[1]\n",
    "eval_data = df_list[1].fillna(0)\n",
    "X_eval = eval_data.drop(columns=['activityID'])\n",
    "y_eval = eval_data['activityID']\n",
    "\n",
    "# Prepare sequences and labels for evaluation\n",
    "X_eval_lstm = []\n",
    "y_eval_lstm = []\n",
    "\n",
    "for i in range(len(X_eval) - timesteps + 1):\n",
    "    X_sequence = X_eval.iloc[i:i + timesteps, :].values\n",
    "    y_label = y_eval.iloc[i + timesteps - 1]  # Get a single label for the sequence\n",
    "    X_eval_lstm.append(X_sequence)\n",
    "    y_eval_lstm.append(y_label)\n",
    "\n",
    "X_eval_lstm = np.array(X_eval_lstm)\n",
    "y_eval_lstm = np.array(y_eval_lstm)\n",
    "\n",
    "# One-hot encode the evaluation labels\n",
    "y_eval_encoded = np.array([to_categorical(label, num_classes=num_classes) for label in y_eval_lstm])\n",
    "y_eval_encoded_flattened = y_eval_encoded.reshape(-1, num_classes)\n",
    "\n",
    "# Evaluate the model on the evaluation data\n",
    "eval_loss, eval_accuracy = model.evaluate(X_eval_lstm, y_eval_encoded_flattened)\n",
    "\n",
    "print(\"Evaluation Loss:\", eval_loss)\n",
    "print(\"Evaluation Accuracy:\", eval_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict labels using the model\n",
    "y_eval_pred_encoded_flattened = model.predict(X_eval_lstm)\n",
    "\n",
    "# Inverse one-hot encode to get predicted labels\n",
    "y_eval_pred_lstm = np.argmax(y_eval_pred_encoded_flattened, axis=1)\n",
    "\n",
    "# Plot the actual and predicted labels over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_eval_lstm, label='Actual Labels', marker='o')\n",
    "plt.plot(y_eval_pred_lstm, label='Predicted Labels', marker='x')\n",
    "\n",
    "# Get unique activityIDs\n",
    "activity_ids = y_eval.unique()\n",
    "\n",
    "# Find and annotate the ranges of each activityID\n",
    "for activity_id in activity_ids:\n",
    "    indices = np.where(y_eval_lstm == activity_id)[0]\n",
    "    if indices.any():\n",
    "        plt.annotate(f'Activity {activity_id}', \n",
    "                     xy=(indices[0], 0), \n",
    "                     xytext=(indices[0], -0.1), \n",
    "                     textcoords='offset points',\n",
    "                     arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"),\n",
    "                     fontsize=10)\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Label')\n",
    "plt.title('Actual vs. Predicted Labels')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
